{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регрессия с функцией потерь Хьюбера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь Хьюбера\n",
    "\n",
    "Функция потерь Хьюбера задает штраф за процедуру оценки. Хьюбер (1964) описал ее как кусочную функцию вида:\n",
    "\n",
    "$${\\displaystyle L_{\\delta }(a)={\\begin{cases}{\\frac {1}{2}}{a^{2}}&{\\text{для }}|a|\\leq \\delta ,\\\\\\delta (|a|-{\\frac {1}{2}}\\delta ),&{\\text{иначе.}}\\end{cases}}}$$\n",
    "Эта функция квадратична для малых значений a, и линейна для больших значений, с одинаковым значением и уклоном для различных участков двух точек где ${\\displaystyle |a|=\\delta }$. Переменную a часто рассматривают как остаток, т.е как разницу между наблюдаемым и предсказанным значением ${\\displaystyle a=y-f(x)}$, поэтому исходное определение может быть расширено до:\n",
    "$$\n",
    "L_\\delta(a,y)=\n",
    "\\begin{cases}\n",
    " \\frac{1}{2}(y - a)^2,                   & |y - a| \\le \\delta, \\\\\n",
    " \\delta\\, |y - a| - \\frac{1}{2}\\delta^2, & \\textrm{иначе.}\n",
    "\\end{cases}\n",
    "$$\n",
    "производная по вектору \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial\\omega}=\\left\\{\\begin{array}{l}X^T(y\\;-\\;\\omega X)\\;,\\;\\;\\;\\;\\;\\left|y\\;-\\;\\omega X\\right|\\leqslant\\delta\\\\X^Tsign\\lbrack y-\\omega X\\rbrack\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy.linalg as la\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея в основе градиентного спуска\n",
    "\n",
    "Пусть имеется некая функция `f`, которая на входе принимает вектор из вещественных чисел и на выходе выдает одно-единственное вещественное число. Вот одна из таких простых функций:\n",
    "\n",
    "```Python\n",
    "def sum_of_squares(v: np.array) -> float:\n",
    "    \"\"\"Вычисляет сумму возведенных в квадрат элементов в v\"\"\"\n",
    "    return np.dot(v, v)\n",
    "```\n",
    "\n",
    "Нередко нам необходимо максимизировать или минимизировать такие функции. Другими словами, нам нужно отыскивать вход `v`, который производит наибольшее (или наименьшее) возможное значение.\n",
    "\n",
    ">Терминология для различных видов градиентного спуска характерна неоднород­\n",
    "ностью. Подход «вычислить градиент для всего набора данных» часто называют пакетным градиентным спуском, при этом некоторые люди используют термин «стохастический градиентный спуск», ссылаясь на мини-пактную версию (среди\n",
    "которых версия с одной точкой за один раз является частным случаем)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация градиентного спуска\n",
    "\n",
    "Модель линейной регрессии для функции потерь `Huber loss`, обучаемая градиентным спуском.\n",
    "\n",
    "Все вычисления должны быть векторизованы, циклы средствами python допускается использовать только для итераций градиентного спуска. В качестве критерия останова необходимо использовать (одновременно):\n",
    "\n",
    "   - проверку на евклидовую норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, задаваемого параметром `tolerance`);\n",
    "   - достижение максимального числа итераций (например, 10000, задаваемого параметром `max_iter`).\n",
    "\n",
    "Необходимо реализовать метод полного и стохастического (в интерпретации `mini-batch`) градиентных спусков, а также поддержать метод `momentum` при помощи параметра `alpha` (способ оценивания градиента должен задаваться при помощи параметра `gd_type`).\n",
    "\n",
    "Чтобы проследить, что оптимизационный процесс действительно сходится, будем использовать атрибут класса `loss_history` — в нём после вызова метода `fit` должны содержаться значения функции потерь для всех итераций, начиная с первой (до совершения первого шага по антиградиенту).\n",
    "\n",
    "Инициализировать веса можно случайным образом или нулевым вектором. Ниже приведён шаблон класса, который должен содержать код реализации модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class HuberReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='full', tolerance=1e-4,\n",
    "                 max_iter=1000, w0=None, alpha=1e-3, eta=1e-2):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0 \n",
    "        self.alpha = alpha\n",
    "        self.w = np.zeros(1)\n",
    "        self.eta = eta\n",
    "        self.loss_history = None\n",
    "    \n",
    "    def calc_loss(self, X, y):\n",
    "        if la.norm(y - np.dot(X, self.w)) <= self.delta:\n",
    "            return 0.5 * la.norm(y - np.dot(X, self.w))\n",
    "        else:\n",
    "            return self.delta * la.norm((y - np.dot(X, self.w) - 0.5 * self.delta), ord=1)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        step_size_0 = 0.05\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        if self.gd_type == 'full':\n",
    "            w_mem = self.w.copy()\n",
    "            h = np.zeros(X.shape[1])\n",
    "            for i in range(self.max_iter):\n",
    "                step_size = step_size_0 / ((i + 1) ** .5)\n",
    "                if la.norm(y - np.dot(X, self.w)) <= self.delta:\n",
    "                    grad = np.dot(X.T, (np.dot(X, self.w) - y)) / y.shape[0]\n",
    "                    self.w -= h\n",
    "                else:\n",
    "                    grad = self.delta * np.dot(X.T,\n",
    "                                            np.sign(np.dot(X, self.w) - y)) / y.shape[0]\n",
    "                    self.w -= h\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                h = self.alpha * h + step_size * grad\n",
    "                if np.abs(la.norm(w_mem) - la.norm(self.w)) < self.tolerance and i != 0:\n",
    "                    break\n",
    "            return self.w\n",
    "\n",
    "        if self.gd_type == 'stochastic':\n",
    "            batch_size = 10\n",
    "            if X.shape[0] < 10:\n",
    "                print('X lenght < 10')\n",
    "                return None\n",
    "            w_mem = self.w.copy()\n",
    "            h = np.zeros(X.shape[1])\n",
    "            for i in range(self.max_iter):\n",
    "                sample = np.random.randint(X.shape[0], size=batch_size)                \n",
    "                step_size = step_size_0 / ((i + 1) ** .5)\n",
    "                if la.norm(y.iloc[sample] - np.dot(X.iloc[sample], self.w)) <= self.delta:\n",
    "                    grad = np.dot(X.iloc[sample].T,\n",
    "                            (np.dot(X.iloc[sample], self.w) - y.iloc[sample])) / batch_size\n",
    "                    self.w -= h\n",
    "                else:\n",
    "                    grad = self.delta * np.dot(X.iloc[sample].T,\n",
    "                     np.sign(np.dot(X.iloc[sample], self.w) - y.iloc[sample])) / batch_size\n",
    "                    self.w -= h\n",
    "                h = self.alpha * h + step_size * grad\n",
    "                self.loss_history.append(self.calc_loss(X.iloc[sample], y.iloc[sample]))\n",
    "                if la.norm(w_mem - self.w) < self.tolerance and i != 0: \n",
    "                    print(\"tolerance break\")\n",
    "        return self.w\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.loss_history = []\n",
    "        self.calc_gradient(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return np.dot(X, self.w)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        return 1 - ((y - np.dot(X, self.w)) ** 2).sum() / ((y - np.mean(y)) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.976270</td>\n",
       "      <td>-3.697815</td>\n",
       "      <td>0.623295</td>\n",
       "      <td>0.524760</td>\n",
       "      <td>7199.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.303787</td>\n",
       "      <td>7.715073</td>\n",
       "      <td>0.886961</td>\n",
       "      <td>0.473862</td>\n",
       "      <td>2466.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.055268</td>\n",
       "      <td>-6.464284</td>\n",
       "      <td>0.618826</td>\n",
       "      <td>1.657394</td>\n",
       "      <td>2969.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.897664</td>\n",
       "      <td>1.335254</td>\n",
       "      <td>0.133461</td>\n",
       "      <td>1.234554</td>\n",
       "      <td>1040.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>-1.526904</td>\n",
       "      <td>-0.196414</td>\n",
       "      <td>0.980580</td>\n",
       "      <td>3.086397</td>\n",
       "      <td>37.469975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f0    f1        f2        f3        f4        f5           f6\n",
       "0  16.99  1.01  0.976270 -3.697815  0.623295  0.524760  7199.992000\n",
       "1  10.34  1.66  4.303787  7.715073  0.886961  0.473862  2466.136700\n",
       "2  21.01  3.50  2.055268 -6.464284  0.618826  1.657394  2969.369100\n",
       "3  23.68  3.31  0.897664  1.335254  0.133461  1.234554  1040.665300\n",
       "4  24.59  3.61 -1.526904 -0.196414  0.980580  3.086397    37.469975"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нормализация предикторов\n",
    "data = data.apply(lambda column: (column - column.mean()) / column.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиение на тренировочный и тестовый датасеты\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data['f1'], test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score: 0.9999207139481568\n",
      "W: [ 7.47531897e-02  9.92153746e-01 -3.91709170e-03 -5.93706464e-03\n",
      " -8.54460756e-05  2.97173690e-04 -3.63442067e-06]\n",
      "CPU times: user 2.2 s, sys: 57.3 ms, total: 2.26 s\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hubreg = HuberReg(alpha=.8)\n",
    "hubreg.fit(X_train, y_train)\n",
    "print(f\"R2-score: {hubreg.score(X_test, y_test)}\")\n",
    "print(f\"W: {hubreg.w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score: 0.9997968983339703\n",
      "W: [ 5.97358461e-02  9.92010984e-01  2.10155648e-02 -3.86417217e-02\n",
      " -8.11426086e-05  7.31855632e-04  4.13167362e-06]\n",
      "CPU times: user 4.61 s, sys: 142 ms, total: 4.76 s\n",
      "Wall time: 5.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hubreg = HuberReg(alpha=.8, gd_type='stochastic')\n",
    "hubreg.fit(X_train, y_train)\n",
    "print(f\"R2-score: {hubreg.score(X_test, y_test)}\")\n",
    "print(f\"W: {hubreg.w}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
